# WebProbe MVP æ„å»ºä»»åŠ¡åˆ—è¡¨

> æœ¬æ–‡æ¡£åŒ…å«æŒ‰é¡ºåºæ‰§è¡Œçš„å°å‹ã€å¯æµ‹è¯•çš„ä»»åŠ¡ã€‚æ¯ä¸ªä»»åŠ¡ä¸“æ³¨äºä¸€ä¸ªé—®é¢˜ï¼Œæœ‰æ˜ç¡®çš„å¼€å§‹å’Œç»“æŸã€‚

---

## ğŸ“‹ ä»»åŠ¡æ‰§è¡Œè¯´æ˜

- âœ… æ¯ä¸ªä»»åŠ¡å®Œæˆåï¼Œè¿è¡ŒéªŒè¯æ­¥éª¤ç¡®ä¿åŠŸèƒ½æ­£å¸¸
- âœ… æŒ‰ç…§ä»»åŠ¡ç¼–å·é¡ºåºæ‰§è¡Œ
- âœ… ä¸è¦è·³è¿‡ä»»ä½•ä»»åŠ¡
- âœ… æ¯ä¸ªä»»åŠ¡å®Œæˆåæ ‡è®°ä¸ºå·²å®Œæˆ

---

## é˜¶æ®µ 1: é¡¹ç›®åŸºç¡€è®¾ç½® (Tasks 1-5)

### Task 1: åˆ›å»ºé¡¹ç›®ä¾èµ–æ–‡ä»¶

**ç›®æ ‡**: åˆ›å»º `requirements.txt` æ–‡ä»¶ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„ Python ä¾èµ–

**å…·ä½“æ­¥éª¤**:

1. åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `requirements.txt`
2. æ·»åŠ ä»¥ä¸‹ä¾èµ–ï¼ˆæŒ‡å®šç‰ˆæœ¬ï¼‰:
   - fastapi==0.104.1
   - uvicorn[standard]==0.24.0
   - sqlalchemy==2.0.23
   - psycopg2-binary==2.9.9
   - pydantic==2.5.0
   - pydantic-settings==2.1.0
   - redis==5.0.1
   - celery==5.3.4
   - requests==2.31.0
   - beautifulsoup4==4.12.2
   - python-dotenv==1.0.0

**éªŒè¯**:

- æ–‡ä»¶å­˜åœ¨äºé¡¹ç›®æ ¹ç›®å½•
- æ–‡ä»¶åŒ…å«æ‰€æœ‰åˆ—å‡ºçš„ä¾èµ–

**å®Œæˆæ ‡å¿—**: `requirements.txt` æ–‡ä»¶å·²åˆ›å»ºå¹¶åŒ…å«æ‰€æœ‰ä¾èµ–

---

### Task 2: åˆ›å»º .gitignore æ–‡ä»¶

**ç›®æ ‡**: åˆ›å»º `.gitignore` æ–‡ä»¶ï¼Œå¿½ç•¥ä¸å¿…è¦çš„æ–‡ä»¶

**å…·ä½“æ­¥éª¤**:

1. åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.gitignore`
2. æ·»åŠ ä»¥ä¸‹å†…å®¹:

   ```
   # Python
   __pycache__/
   *.py[cod]
   *$py.class
   *.so
   .Python
   env/
   venv/
   ENV/

   # Environment
   .env
   .env.local

   # IDE
   .vscode/
   .idea/
   *.swp
   *.swo

   # Database
   *.db
   *.sqlite

   # Logs
   *.log
   logs/

   # Testing
   .pytest_cache/
   .coverage
   htmlcov/

   # Celery
   celerybeat-schedule
   celerybeat.pid
   ```

**éªŒè¯**:

- `.gitignore` æ–‡ä»¶å­˜åœ¨
- æ–‡ä»¶åŒ…å«å¸¸è§çš„ Python å’Œé¡¹ç›®ç›¸å…³çš„å¿½ç•¥è§„åˆ™

**å®Œæˆæ ‡å¿—**: `.gitignore` æ–‡ä»¶å·²åˆ›å»º

---

### Task 3: åˆ›å»ºç¯å¢ƒå˜é‡ç¤ºä¾‹æ–‡ä»¶

**ç›®æ ‡**: åˆ›å»º `.env.example` æ–‡ä»¶ä½œä¸ºç¯å¢ƒå˜é‡æ¨¡æ¿

**å…·ä½“æ­¥éª¤**:

1. åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env.example`
2. æ·»åŠ ä»¥ä¸‹å†…å®¹:

   ```bash
   # Database
   DATABASE_URL=postgresql://webprobe:secret@localhost:5432/webprobe_db

   # Redis
   REDIS_HOST=localhost
   REDIS_PORT=6379
   REDIS_URL=redis://localhost:6379/0

   # Celery
   CELERY_BROKER_URL=redis://localhost:6379/0
   CELERY_RESULT_BACKEND=redis://localhost:6379/1

   # Application
   APP_NAME=WebProbe
   DEBUG=true
   LOG_LEVEL=INFO

   # Cache
   CACHE_TTL=300

   # Scraper
   REQUEST_TIMEOUT=30
   MAX_RETRIES=3
   USER_AGENT=WebProbe/1.0
   ```

**éªŒè¯**:

- `.env.example` æ–‡ä»¶å­˜åœ¨
- åŒ…å«æ‰€æœ‰å¿…è¦çš„é…ç½®é¡¹

**å®Œæˆæ ‡å¿—**: `.env.example` æ–‡ä»¶å·²åˆ›å»º

---

### Task 4: åˆ›å»ºåŸºç¡€ç›®å½•ç»“æ„

**ç›®æ ‡**: åˆ›å»ºé¡¹ç›®æ‰€éœ€çš„æ‰€æœ‰ç©ºç›®å½•å’Œ `__init__.py` æ–‡ä»¶

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»ºä»¥ä¸‹ç›®å½•ç»“æ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰:

   ```
   app/
   app/api/
   app/api/v1/
   app/api/v1/endpoints/
   app/models/
   app/schemas/
   app/services/
   app/repositories/
   app/celery_app/
   app/celery_app/tasks/
   app/core/
   app/utils/
   tests/
   tests/unit/
   tests/integration/
   ```

2. åœ¨æ¯ä¸ª Python åŒ…ç›®å½•ä¸­åˆ›å»ºç©ºçš„ `__init__.py` æ–‡ä»¶

**éªŒè¯**:

- è¿è¡Œ `find app -type f -name "__init__.py"` åº”è¯¥åˆ—å‡ºæ‰€æœ‰ `__init__.py` æ–‡ä»¶
- æ‰€æœ‰ç›®å½•å­˜åœ¨

**å®Œæˆæ ‡å¿—**: ç›®å½•ç»“æ„å·²åˆ›å»ºï¼Œæ‰€æœ‰ `__init__.py` æ–‡ä»¶å­˜åœ¨

---

### Task 5: åˆ›å»ºé…ç½®ç®¡ç†æ¨¡å— (app/config.py)

**ç›®æ ‡**: åˆ›å»ºé…ç½®ç®¡ç†ç±»ï¼Œä½¿ç”¨ Pydantic Settings åŠ è½½ç¯å¢ƒå˜é‡

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `app/config.py`
2. å®ç° `Settings` ç±»:
   - ç»§æ‰¿ `BaseSettings`
   - å®šä¹‰æ‰€æœ‰é…ç½®å­—æ®µï¼ˆæ•°æ®åº“ã€Redisã€Celeryã€åº”ç”¨ç­‰ï¼‰
   - é…ç½®ä» `.env` æ–‡ä»¶åŠ è½½
   - åˆ›å»ºå…¨å±€ `settings` å®ä¾‹

**ä»£ç ç»“æ„**:

```python
from pydantic_settings import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    # Database
    DATABASE_URL: str

    # Redis
    REDIS_HOST: str = "localhost"
    REDIS_PORT: int = 6379
    REDIS_URL: str

    # Celery
    CELERY_BROKER_URL: str
    CELERY_RESULT_BACKEND: str

    # Application
    APP_NAME: str = "WebProbe"
    DEBUG: bool = False
    LOG_LEVEL: str = "INFO"

    # Cache
    CACHE_TTL: int = 300

    # Scraper
    REQUEST_TIMEOUT: int = 30
    MAX_RETRIES: int = 3
    USER_AGENT: str = "WebProbe/1.0"

    class Config:
        env_file = ".env"
        case_sensitive = False

settings = Settings()
```

**éªŒè¯**:

- åˆ›å»º `.env` æ–‡ä»¶ï¼ˆå¤åˆ¶è‡ª `.env.example`ï¼‰
- åœ¨ Python REPL ä¸­è¿è¡Œ: `from app.config import settings; print(settings.APP_NAME)`
- åº”è¯¥æˆåŠŸæ‰“å°é…ç½®å€¼

**å®Œæˆæ ‡å¿—**: `app/config.py` å·²åˆ›å»ºï¼Œå¯ä»¥æˆåŠŸå¯¼å…¥å’Œè¯»å–é…ç½®

---

## é˜¶æ®µ 2: æ•°æ®åº“å±‚ (Tasks 6-11)

### Task 6: åˆ›å»ºæ•°æ®åº“åŸºç¡€æ¨¡å‹ (app/models/base.py)

**ç›®æ ‡**: åˆ›å»º SQLAlchemy å£°æ˜å¼åŸºç±»

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `app/models/base.py`
2. å®šä¹‰ `Base` ç±»å’Œé€šç”¨çš„åŸºç¡€æ¨¡å‹å­—æ®µ

**ä»£ç ç»“æ„**:

```python
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy import Column, DateTime
from datetime import datetime
import uuid
from sqlalchemy.dialects.postgresql import UUID

Base = declarative_base()

class TimestampMixin:
    """Mixin for created_at and updated_at timestamps"""
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)

class UUIDMixin:
    """Mixin for UUID primary key"""
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
```

**éªŒè¯**:

- æ–‡ä»¶å¯ä»¥æˆåŠŸå¯¼å…¥: `from app.models.base import Base`
- æ²¡æœ‰è¯­æ³•é”™è¯¯

**å®Œæˆæ ‡å¿—**: `app/models/base.py` å·²åˆ›å»ºå¹¶å¯å¯¼å…¥

---

### Task 7: åˆ›å»º Task æ¨¡å‹ (app/models/task.py)

**ç›®æ ‡**: å®šä¹‰ Task æ•°æ®åº“æ¨¡å‹

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `app/models/task.py`
2. å®šä¹‰ `TaskStatus` æšä¸¾
3. å®šä¹‰ `Task` æ¨¡å‹ç±»

**ä»£ç ç»“æ„**:

```python
from sqlalchemy import Column, String, DateTime, Enum, Text
from sqlalchemy.dialects.postgresql import UUID
from app.models.base import Base, TimestampMixin, UUIDMixin
import enum
from typing import Optional

class TaskStatus(str, enum.Enum):
    PENDING = "PENDING"
    PROCESSING = "PROCESSING"
    SUCCESS = "SUCCESS"
    FAILED = "FAILED"

class Task(Base, UUIDMixin, TimestampMixin):
    __tablename__ = "tasks"

    url = Column(String(2048), nullable=False, index=True)
    status = Column(Enum(TaskStatus), default=TaskStatus.PENDING, nullable=False, index=True)
    started_at = Column(DateTime, nullable=True)
    completed_at = Column(DateTime, nullable=True)
    error_message = Column(Text, nullable=True)
    result_id = Column(UUID(as_uuid=True), nullable=True)

    def __repr__(self):
        return f"<Task(id={self.id}, url={self.url}, status={self.status})>"
```

**éªŒè¯**:

- æˆåŠŸå¯¼å…¥: `from app.models.task import Task, TaskStatus`
- æšä¸¾å€¼å¯è®¿é—®: `print(TaskStatus.PENDING)`

**å®Œæˆæ ‡å¿—**: `app/models/task.py` å·²åˆ›å»ºï¼ŒTask æ¨¡å‹å®šä¹‰å®Œæ•´

---

### Task 8: åˆ›å»º Result æ¨¡å‹ (app/models/result.py)

**ç›®æ ‡**: å®šä¹‰ Result æ•°æ®åº“æ¨¡å‹ï¼Œå­˜å‚¨æŠ“å–ç»“æœ

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `app/models/result.py`
2. å®šä¹‰ `Result` æ¨¡å‹ç±»

**ä»£ç ç»“æ„**:

```python
from sqlalchemy import Column, String, Text, DateTime, ForeignKey
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.orm import relationship
from app.models.base import Base, UUIDMixin, TimestampMixin
from datetime import datetime

class Result(Base, UUIDMixin, TimestampMixin):
    __tablename__ = "results"

    task_id = Column(UUID(as_uuid=True), ForeignKey("tasks.id"), nullable=False, unique=True)
    title = Column(String(500), nullable=True)
    description = Column(Text, nullable=True)
    links = Column(JSONB, nullable=True, default=list)
    text_content = Column(Text, nullable=True)
    extra_data = Column(JSONB, nullable=True, default=dict)
    scraped_at = Column(DateTime, default=datetime.utcnow, nullable=False)

    def __repr__(self):
        return f"<Result(id={self.id}, task_id={self.task_id}, title={self.title})>"
```

**éªŒè¯**:

- æˆåŠŸå¯¼å…¥: `from app.models.result import Result`
- æ²¡æœ‰å¯¼å…¥é”™è¯¯

**å®Œæˆæ ‡å¿—**: `app/models/result.py` å·²åˆ›å»º

---

### Task 9: æ›´æ–° models/**init**.py

**ç›®æ ‡**: åœ¨ models åŒ…çš„ `__init__.py` ä¸­å¯¼å‡ºæ‰€æœ‰æ¨¡å‹

**å…·ä½“æ­¥éª¤**:

1. ç¼–è¾‘ `app/models/__init__.py`
2. å¯¼å…¥å¹¶å¯¼å‡ºæ‰€æœ‰æ¨¡å‹

**ä»£ç å†…å®¹**:

```python
from app.models.base import Base
from app.models.task import Task, TaskStatus
from app.models.result import Result

__all__ = ["Base", "Task", "TaskStatus", "Result"]
```

**éªŒè¯**:

- æˆåŠŸå¯¼å…¥: `from app.models import Task, Result, Base, TaskStatus`

**å®Œæˆæ ‡å¿—**: æ‰€æœ‰æ¨¡å‹å¯ä»¥ä» `app.models` åŒ…å¯¼å…¥

---

### Task 10: åˆ›å»ºæ•°æ®åº“è¿æ¥æ¨¡å— (app/core/database.py)

**ç›®æ ‡**: åˆ›å»ºæ•°æ®åº“å¼•æ“ã€ä¼šè¯å·¥å‚å’Œä¾èµ–æ³¨å…¥å‡½æ•°

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `app/core/database.py`
2. åˆ›å»ºæ•°æ®åº“å¼•æ“
3. åˆ›å»ºä¼šè¯å·¥å‚
4. å®ç° `get_db()` ä¾èµ–å‡½æ•°

**ä»£ç ç»“æ„**:

```python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, Session
from app.config import settings
from typing import Generator

# Create engine
engine = create_engine(
    settings.DATABASE_URL,
    pool_pre_ping=True,
    pool_size=10,
    max_overflow=20,
    echo=settings.DEBUG
)

# Create session factory
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def get_db() -> Generator[Session, None, None]:
    """
    Dependency function that yields a database session.
    Used with FastAPI's Depends().
    """
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def init_db():
    """
    Initialize database tables.
    Creates all tables defined in models.
    """
    from app.models import Base
    Base.metadata.create_all(bind=engine)
```

**éªŒè¯**:

- æˆåŠŸå¯¼å…¥: `from app.core.database import engine, get_db, init_db`
- æ³¨æ„: æš‚æ—¶ä¸è¦è¿è¡Œ `init_db()`ï¼Œå› ä¸ºæ•°æ®åº“å¯èƒ½è¿˜æœªå‡†å¤‡å¥½

**å®Œæˆæ ‡å¿—**: `app/core/database.py` å·²åˆ›å»º

---

### Task 11: åˆ›å»ºæ•°æ®åº“åˆå§‹åŒ–è„šæœ¬ (scripts/init_db.py)

**ç›®æ ‡**: åˆ›å»ºç”¨äºåˆå§‹åŒ–æ•°æ®åº“è¡¨çš„ç‹¬ç«‹è„šæœ¬

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `scripts/` ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
2. åˆ›å»º `scripts/init_db.py`

**ä»£ç ç»“æ„**:

```python
#!/usr/bin/env python3
"""
Database initialization script.
Run this to create all database tables.
"""

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.core.database import init_db, engine
from app.models import Base

def main():
    print("Creating database tables...")
    try:
        Base.metadata.create_all(bind=engine)
        print("âœ… Database tables created successfully!")
    except Exception as e:
        print(f"âŒ Error creating tables: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
```

**éªŒè¯**:

- æ–‡ä»¶å­˜åœ¨ä¸”å¯æ‰§è¡Œ
- å¯ä»¥è¿è¡Œï¼ˆå½“æ•°æ®åº“å¯ç”¨æ—¶ï¼‰: `python scripts/init_db.py`

**å®Œæˆæ ‡å¿—**: æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬å·²åˆ›å»º

---

## é˜¶æ®µ 3: Pydantic Schemas (Tasks 12-14)

### Task 12: åˆ›å»º Task Schemas (app/schemas/task.py)

**ç›®æ ‡**: å®šä¹‰ Task ç›¸å…³çš„ Pydantic æ¨¡å‹ç”¨äº API è¯·æ±‚/å“åº”

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `app/schemas/task.py`
2. å®šä¹‰ä»¥ä¸‹ Schema:
   - `TaskCreate`: åˆ›å»ºä»»åŠ¡çš„è¯·æ±‚ä½“
   - `TaskResponse`: ä»»åŠ¡çš„å“åº”æ¨¡å‹
   - `TaskListResponse`: ä»»åŠ¡åˆ—è¡¨å“åº”

**ä»£ç ç»“æ„**:

```python
from pydantic import BaseModel, HttpUrl, Field
from datetime import datetime
from typing import Optional
from uuid import UUID
from app.models.task import TaskStatus

class TaskCreate(BaseModel):
    """Schema for creating a new task"""
    url: HttpUrl = Field(..., description="URL to scrape")

    class Config:
        json_schema_extra = {
            "example": {
                "url": "https://example.com"
            }
        }

class TaskResponse(BaseModel):
    """Schema for task response"""
    id: UUID
    url: str
    status: TaskStatus
    created_at: datetime
    updated_at: datetime
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    error_message: Optional[str] = None
    result_id: Optional[UUID] = None

    class Config:
        from_attributes = True

class TaskListResponse(BaseModel):
    """Schema for paginated task list"""
    tasks: list[TaskResponse]
    total: int
    page: int
    page_size: int
```

**éªŒè¯**:

- æˆåŠŸå¯¼å…¥: `from app.schemas.task import TaskCreate, TaskResponse`
- å¯ä»¥åˆ›å»ºå®ä¾‹: `task = TaskCreate(url="https://example.com")`

**å®Œæˆæ ‡å¿—**: Task schemas å·²åˆ›å»ºå¹¶å¯å¯¼å…¥

---

### Task 13: åˆ›å»º Result Schemas (app/schemas/result.py)

**ç›®æ ‡**: å®šä¹‰ Result ç›¸å…³çš„ Pydantic æ¨¡å‹

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `app/schemas/result.py`
2. å®šä¹‰ `ResultResponse` Schema

**ä»£ç ç»“æ„**:

```python
from pydantic import BaseModel
from datetime import datetime
from typing import Optional, Dict, List, Any
from uuid import UUID

class ResultResponse(BaseModel):
    """Schema for scraping result response"""
    id: UUID
    task_id: UUID
    title: Optional[str] = None
    description: Optional[str] = None
    links: List[str] = []
    text_content: Optional[str] = None
    extra_data: Dict[str, Any] = {}
    scraped_at: datetime
    created_at: datetime

    class Config:
        from_attributes = True
```

**éªŒè¯**:

- æˆåŠŸå¯¼å…¥: `from app.schemas.result import ResultResponse`

**å®Œæˆæ ‡å¿—**: Result schema å·²åˆ›å»º

---

### Task 14: æ›´æ–° schemas/**init**.py

**ç›®æ ‡**: å¯¼å‡ºæ‰€æœ‰ schemas

**å…·ä½“æ­¥éª¤**:

1. ç¼–è¾‘ `app/schemas/__init__.py`

**ä»£ç å†…å®¹**:

```python
from app.schemas.task import TaskCreate, TaskResponse, TaskListResponse
from app.schemas.result import ResultResponse

__all__ = [
    "TaskCreate",
    "TaskResponse",
    "TaskListResponse",
    "ResultResponse",
]
```

**éªŒè¯**:

- æˆåŠŸå¯¼å…¥: `from app.schemas import TaskCreate, TaskResponse, ResultResponse`

**å®Œæˆæ ‡å¿—**: Schemas å¯ä»¥ç»Ÿä¸€å¯¼å…¥

---

## é˜¶æ®µ 4: Repository å±‚ (Tasks 15-17)

### Task 15: åˆ›å»ºåŸºç¡€ Repository (app/repositories/base.py)

**ç›®æ ‡**: åˆ›å»ºé€šç”¨çš„ Repository åŸºç±»

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `app/repositories/base.py`
2. å®ç° `BaseRepository` ç±»

**ä»£ç ç»“æ„**:

```python
from typing import Generic, TypeVar, Type, Optional, List
from sqlalchemy.orm import Session
from app.models.base import Base

ModelType = TypeVar("ModelType", bound=Base)

class BaseRepository(Generic[ModelType]):
    """Base repository with common CRUD operations"""

    def __init__(self, model: Type[ModelType], db: Session):
        self.model = model
        self.db = db

    def get_by_id(self, id: any) -> Optional[ModelType]:
        """Get a record by ID"""
        return self.db.query(self.model).filter(self.model.id == id).first()

    def get_all(self, skip: int = 0, limit: int = 100) -> List[ModelType]:
        """Get all records with pagination"""
        return self.db.query(self.model).offset(skip).limit(limit).all()

    def create(self, obj_in: dict) -> ModelType:
        """Create a new record"""
        db_obj = self.model(**obj_in)
        self.db.add(db_obj)
        self.db.commit()
        self.db.refresh(db_obj)
        return db_obj

    def update(self, db_obj: ModelType, obj_in: dict) -> ModelType:
        """Update an existing record"""
        for field, value in obj_in.items():
            setattr(db_obj, field, value)
        self.db.commit()
        self.db.refresh(db_obj)
        return db_obj

    def delete(self, id: any) -> bool:
        """Delete a record by ID"""
        obj = self.get_by_id(id)
        if obj:
            self.db.delete(obj)
            self.db.commit()
            return True
        return False
```

**éªŒè¯**:

- æˆåŠŸå¯¼å…¥: `from app.repositories.base import BaseRepository`

**å®Œæˆæ ‡å¿—**: åŸºç¡€ Repository å·²åˆ›å»º

---

### Task 16: åˆ›å»º Task Repository (app/repositories/task_repository.py)

**ç›®æ ‡**: åˆ›å»ºä¸“é—¨å¤„ç† Task çš„ Repository

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `app/repositories/task_repository.py`
2. ç»§æ‰¿ `BaseRepository` å¹¶æ·»åŠ  Task ç‰¹å®šçš„æ–¹æ³•

**ä»£ç ç»“æ„**:

```python
from typing import Optional, List
from sqlalchemy.orm import Session
from uuid import UUID
from app.repositories.base import BaseRepository
from app.models.task import Task, TaskStatus

class TaskRepository(BaseRepository[Task]):
    """Repository for Task operations"""

    def __init__(self, db: Session):
        super().__init__(Task, db)

    def get_by_url(self, url: str) -> Optional[Task]:
        """Get task by URL"""
        return self.db.query(Task).filter(Task.url == url).first()

    def get_by_status(self, status: TaskStatus, skip: int = 0, limit: int = 100) -> List[Task]:
        """Get tasks by status"""
        return self.db.query(Task).filter(Task.status == status).offset(skip).limit(limit).all()

    def update_status(self, task_id: UUID, status: TaskStatus, error_message: Optional[str] = None) -> Optional[Task]:
        """Update task status"""
        task = self.get_by_id(task_id)
        if task:
            update_data = {"status": status}
            if error_message:
                update_data["error_message"] = error_message
            return self.update(task, update_data)
        return None

    def count_by_status(self, status: TaskStatus) -> int:
        """Count tasks by status"""
        return self.db.query(Task).filter(Task.status == status).count()

    def get_recent(self, limit: int = 10) -> List[Task]:
        """Get most recent tasks"""
        return self.db.query(Task).order_by(Task.created_at.desc()).limit(limit).all()
```

**éªŒè¯**:

- æˆåŠŸå¯¼å…¥: `from app.repositories.task_repository import TaskRepository`

**å®Œæˆæ ‡å¿—**: Task Repository å·²åˆ›å»º

---

### Task 17: æ›´æ–° repositories/**init**.py

**ç›®æ ‡**: å¯¼å‡ºæ‰€æœ‰ repositories

**å…·ä½“æ­¥éª¤**:

1. ç¼–è¾‘ `app/repositories/__init__.py`

**ä»£ç å†…å®¹**:

```python
from app.repositories.base import BaseRepository
from app.repositories.task_repository import TaskRepository

__all__ = ["BaseRepository", "TaskRepository"]
```

**éªŒè¯**:

- æˆåŠŸå¯¼å…¥: `from app.repositories import TaskRepository`

**å®Œæˆæ ‡å¿—**: Repositories å¯ç»Ÿä¸€å¯¼å…¥

---

## é˜¶æ®µ 5: åŸºç¡€ API (Tasks 18-22)

### Task 18: åˆ›å»º API ä¾èµ–æ³¨å…¥ (app/api/v1/dependencies.py)

**ç›®æ ‡**: åˆ›å»º API ç«¯ç‚¹ä½¿ç”¨çš„ä¾èµ–æ³¨å…¥å‡½æ•°

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `app/api/v1/dependencies.py`
2. å®šä¹‰å¸¸ç”¨çš„ä¾èµ–å‡½æ•°

**ä»£ç ç»“æ„**:

```python
from typing import Generator
from fastapi import Depends
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.repositories.task_repository import TaskRepository

def get_task_repository(db: Session = Depends(get_db)) -> TaskRepository:
    """Dependency to get TaskRepository instance"""
    return TaskRepository(db)
```

**éªŒè¯**:

- æˆåŠŸå¯¼å…¥: `from app.api.v1.dependencies import get_task_repository`

**å®Œæˆæ ‡å¿—**: API ä¾èµ–å·²åˆ›å»º

---

### Task 19: åˆ›å»ºå¥åº·æ£€æŸ¥ç«¯ç‚¹ (app/api/v1/endpoints/health.py)

**ç›®æ ‡**: åˆ›å»ºå¥åº·æ£€æŸ¥ API ç«¯ç‚¹

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `app/api/v1/endpoints/health.py`
2. å®ç° `/health` ç«¯ç‚¹

**ä»£ç ç»“æ„**:

```python
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from app.core.database import get_db
from app.config import settings

router = APIRouter()

@router.get("/health", tags=["Health"])
async def health_check(db: Session = Depends(get_db)):
    """
    Health check endpoint.
    Verifies database connectivity and service status.
    """
    try:
        # Test database connection
        db.execute("SELECT 1")

        return {
            "status": "healthy",
            "service": settings.APP_NAME,
            "database": "connected"
        }
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"Service unhealthy: {str(e)}")
```

**éªŒè¯**:

- æ–‡ä»¶å¯ä»¥å¯¼å…¥
- Router å®šä¹‰æ­£ç¡®

**å®Œæˆæ ‡å¿—**: å¥åº·æ£€æŸ¥ç«¯ç‚¹å·²åˆ›å»º

---

### Task 20: åˆ›å»ºä»»åŠ¡ API ç«¯ç‚¹ - åˆ›å»ºä»»åŠ¡ (app/api/v1/endpoints/tasks.py - Part 1)

**ç›®æ ‡**: å®ç° `POST /tasks` ç«¯ç‚¹ç”¨äºåˆ›å»ºæ–°ä»»åŠ¡

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `app/api/v1/endpoints/tasks.py`
2. å®ç°åˆ›å»ºä»»åŠ¡çš„ç«¯ç‚¹

**ä»£ç ç»“æ„**:

```python
from fastapi import APIRouter, Depends, HTTPException, status
from uuid import UUID
from app.schemas.task import TaskCreate, TaskResponse
from app.repositories.task_repository import TaskRepository
from app.api.v1.dependencies import get_task_repository

router = APIRouter()

@router.post("/tasks", response_model=TaskResponse, status_code=status.HTTP_201_CREATED, tags=["Tasks"])
async def create_task(
    task_in: TaskCreate,
    task_repo: TaskRepository = Depends(get_task_repository)
):
    """
    Create a new scraping task.

    - **url**: The URL to scrape

    Returns the created task with status PENDING.
    """
    try:
        # Create task data
        task_data = {
            "url": str(task_in.url),
            "status": "PENDING"
        }

        # Create task in database
        task = task_repo.create(task_data)

        # TODO: Later we will trigger Celery task here

        return task
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to create task: {str(e)}"
        )
```

**éªŒè¯**:

- æ–‡ä»¶å¯ä»¥æˆåŠŸå¯¼å…¥
- Router å®šä¹‰æ­£ç¡®

**å®Œæˆæ ‡å¿—**: åˆ›å»ºä»»åŠ¡ç«¯ç‚¹å·²å®ç°

---

### Task 21: åˆ›å»ºä»»åŠ¡ API ç«¯ç‚¹ - æŸ¥è¯¢ä»»åŠ¡ (app/api/v1/endpoints/tasks.py - Part 2)

**ç›®æ ‡**: åœ¨åŒä¸€æ–‡ä»¶ä¸­æ·»åŠ æŸ¥è¯¢ä»»åŠ¡çš„ç«¯ç‚¹

**å…·ä½“æ­¥éª¤**:

1. ç¼–è¾‘ `app/api/v1/endpoints/tasks.py`
2. æ·»åŠ ä»¥ä¸‹ç«¯ç‚¹:
   - `GET /tasks/{task_id}` - è·å–å•ä¸ªä»»åŠ¡
   - `GET /tasks` - è·å–ä»»åŠ¡åˆ—è¡¨

**æ·»åŠ çš„ä»£ç **:

```python
from typing import Optional
from app.schemas.task import TaskListResponse
from app.models.task import TaskStatus

@router.get("/tasks/{task_id}", response_model=TaskResponse, tags=["Tasks"])
async def get_task(
    task_id: UUID,
    task_repo: TaskRepository = Depends(get_task_repository)
):
    """
    Get a specific task by ID.

    Returns task details including current status.
    """
    task = task_repo.get_by_id(task_id)
    if not task:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Task {task_id} not found"
        )
    return task

@router.get("/tasks", response_model=TaskListResponse, tags=["Tasks"])
async def list_tasks(
    skip: int = 0,
    limit: int = 10,
    status_filter: Optional[TaskStatus] = None,
    task_repo: TaskRepository = Depends(get_task_repository)
):
    """
    List tasks with pagination and optional status filter.

    - **skip**: Number of records to skip (default: 0)
    - **limit**: Maximum number of records to return (default: 10)
    - **status_filter**: Optional status filter (PENDING, PROCESSING, SUCCESS, FAILED)
    """
    if status_filter:
        tasks = task_repo.get_by_status(status_filter, skip, limit)
        total = task_repo.count_by_status(status_filter)
    else:
        tasks = task_repo.get_all(skip, limit)
        total = len(tasks)  # Simple count, not efficient for large datasets

    return {
        "tasks": tasks,
        "total": total,
        "page": skip // limit + 1 if limit > 0 else 1,
        "page_size": limit
    }
```

**éªŒè¯**:

- æ–‡ä»¶å¯ä»¥æˆåŠŸå¯¼å…¥
- æ‰€æœ‰ç«¯ç‚¹å®šä¹‰æ­£ç¡®

**å®Œæˆæ ‡å¿—**: ä»»åŠ¡æŸ¥è¯¢ç«¯ç‚¹å·²å®ç°

---

### Task 22: åˆ›å»º API è·¯ç”±èšåˆ (app/api/v1/**init**.py å’Œ app/main.py)

**ç›®æ ‡**: èšåˆæ‰€æœ‰ v1 API è·¯ç”±å¹¶åˆ›å»º FastAPI åº”ç”¨

**å…·ä½“æ­¥éª¤**:

**æ­¥éª¤ 1**: ç¼–è¾‘ `app/api/v1/__init__.py`

```python
from fastapi import APIRouter
from app.api.v1.endpoints import tasks, health

api_router = APIRouter()

# Include all endpoint routers
api_router.include_router(health.router, prefix="", tags=["Health"])
api_router.include_router(tasks.router, prefix="", tags=["Tasks"])
```

**æ­¥éª¤ 2**: åˆ›å»º `app/main.py`

```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from app.config import settings
from app.api.v1 import api_router

# Create FastAPI app
app = FastAPI(
    title=settings.APP_NAME,
    description="Async Web Scraping and Analysis Platform",
    version="1.0.0",
    debug=settings.DEBUG
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, replace with specific origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include API router
app.include_router(api_router, prefix="/api/v1")

@app.get("/", tags=["Root"])
async def root():
    """Root endpoint"""
    return {
        "message": "Welcome to WebProbe API",
        "docs": "/docs",
        "health": "/api/v1/health"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**éªŒè¯**:

- å¯åŠ¨åº”ç”¨: `uvicorn app.main:app --reload`
- è®¿é—® `http://localhost:8000/docs` æŸ¥çœ‹ API æ–‡æ¡£
- æµ‹è¯•å¥åº·æ£€æŸ¥: `curl http://localhost:8000/api/v1/health`

**å®Œæˆæ ‡å¿—**: FastAPI åº”ç”¨å·²åˆ›å»ºï¼ŒAPI ç«¯ç‚¹å¯è®¿é—®

---

## é˜¶æ®µ 6: Celery é›†æˆ (Tasks 23-26)

### Task 23: åˆ›å»º Celery é…ç½® (app/celery_app/celery_config.py)

**ç›®æ ‡**: é…ç½® Celery åº”ç”¨

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `app/celery_app/celery_config.py`
2. é…ç½® Celery broker å’Œ backend

**ä»£ç ç»“æ„**:

```python
from celery import Celery
from app.config import settings

# Create Celery instance
celery_app = Celery(
    "webprobe",
    broker=settings.CELERY_BROKER_URL,
    backend=settings.CELERY_RESULT_BACKEND,
    include=["app.celery_app.tasks.scrape_task"]
)

# Celery configuration
celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
    task_track_started=True,
    task_time_limit=300,  # 5 minutes
    task_soft_time_limit=240,  # 4 minutes
    worker_prefetch_multiplier=1,
    worker_max_tasks_per_child=1000,
)

# Optional: Configure task routes
celery_app.conf.task_routes = {
    "app.celery_app.tasks.scrape_task.*": {"queue": "scraping"},
}
```

**éªŒè¯**:

- æˆåŠŸå¯¼å…¥: `from app.celery_app.celery_config import celery_app`

**å®Œæˆæ ‡å¿—**: Celery é…ç½®å·²åˆ›å»º

---

### Task 24: åˆ›å»ºåŸºç¡€çˆ¬è™«æœåŠ¡ (app/services/scraper_service.py)

**ç›®æ ‡**: å®ç°ç®€å•çš„ç½‘é¡µæŠ“å–é€»è¾‘

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `app/services/scraper_service.py`
2. å®ç°åŸºç¡€çš„ç½‘é¡µæŠ“å–åŠŸèƒ½

**ä»£ç ç»“æ„**:

```python
import requests
from bs4 import BeautifulSoup
from typing import Dict, List, Optional
from app.config import settings
import logging

logger = logging.getLogger(__name__)

class ScraperService:
    """Service for web scraping operations"""

    def __init__(self):
        self.timeout = settings.REQUEST_TIMEOUT
        self.user_agent = settings.USER_AGENT
        self.max_retries = settings.MAX_RETRIES

    def scrape_url(self, url: str) -> Dict:
        """
        Scrape a URL and extract content.

        Returns:
            Dictionary with title, links, text_content, and extra_data
        """
        try:
            # Make HTTP request
            headers = {"User-Agent": self.user_agent}
            response = requests.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()

            # Parse HTML
            soup = BeautifulSoup(response.content, "html.parser")

            # Extract data
            result = {
                "title": self._extract_title(soup),
                "description": self._extract_description(soup),
                "links": self._extract_links(soup, url),
                "text_content": self._extract_text(soup),
                "extra_data": {
                    "status_code": response.status_code,
                    "content_type": response.headers.get("Content-Type"),
                    "content_length": len(response.content)
                }
            }

            return result

        except requests.RequestException as e:
            logger.error(f"Failed to scrape {url}: {e}")
            raise Exception(f"Scraping failed: {str(e)}")

    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """Extract page title"""
        title_tag = soup.find("title")
        return title_tag.get_text(strip=True) if title_tag else None

    def _extract_description(self, soup: BeautifulSoup) -> Optional[str]:
        """Extract meta description"""
        meta_desc = soup.find("meta", attrs={"name": "description"})
        return meta_desc.get("content") if meta_desc else None

    def _extract_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """Extract all links from page"""
        links = []
        for link in soup.find_all("a", href=True):
            href = link["href"]
            # Simple link extraction (can be improved with urljoin for relative URLs)
            if href.startswith("http"):
                links.append(href)
        return links[:100]  # Limit to first 100 links

    def _extract_text(self, soup: BeautifulSoup) -> str:
        """Extract main text content"""
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()

        # Get text
        text = soup.get_text(separator=" ", strip=True)

        # Limit text length
        return text[:5000]  # First 5000 characters
```

**éªŒè¯**:

- æˆåŠŸå¯¼å…¥: `from app.services.scraper_service import ScraperService`
- å¯ä»¥åˆ›å»ºå®ä¾‹: `scraper = ScraperService()`

**å®Œæˆæ ‡å¿—**: çˆ¬è™«æœåŠ¡å·²åˆ›å»º

---

### Task 25: åˆ›å»º Celery æŠ“å–ä»»åŠ¡ (app/celery_app/tasks/scrape_task.py)

**ç›®æ ‡**: å®ç°å¼‚æ­¥æŠ“å–ä»»åŠ¡

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `app/celery_app/tasks/scrape_task.py`
2. å®ç°æŠ“å–ä»»åŠ¡é€»è¾‘

**ä»£ç ç»“æ„**:

```python
from celery import Task
from app.celery_app.celery_config import celery_app
from app.services.scraper_service import ScraperService
from app.core.database import SessionLocal
from app.repositories.task_repository import TaskRepository
from app.models.task import TaskStatus
from app.models.result import Result
from datetime import datetime
from uuid import UUID
import logging

logger = logging.getLogger(__name__)

class ScraperTask(Task):
    """Custom task class with database session handling"""

    def __call__(self, *args, **kwargs):
        return super().__call__(*args, **kwargs)

@celery_app.task(bind=True, base=ScraperTask, name="scrape_website", max_retries=3)
def scrape_website_task(self, task_id: str, url: str):
    """
    Asynchronous task to scrape a website.

    Args:
        task_id: UUID of the task in database
        url: URL to scrape
    """
    db = SessionLocal()
    task_repo = TaskRepository(db)

    try:
        # Convert string ID to UUID
        task_uuid = UUID(task_id)

        # Update task status to PROCESSING
        logger.info(f"Starting scrape task for {url}")
        task_repo.update_status(task_uuid, TaskStatus.PROCESSING)

        # Update started_at timestamp
        task = task_repo.get_by_id(task_uuid)
        if task:
            task_repo.update(task, {"started_at": datetime.utcnow()})

        # Perform scraping
        scraper = ScraperService()
        scrape_result = scraper.scrape_url(url)

        # Save result to database
        result = Result(
            task_id=task_uuid,
            title=scrape_result.get("title"),
            description=scrape_result.get("description"),
            links=scrape_result.get("links", []),
            text_content=scrape_result.get("text_content"),
            extra_data=scrape_result.get("extra_data", {}),
            scraped_at=datetime.utcnow()
        )
        db.add(result)
        db.commit()
        db.refresh(result)

        # Update task status to SUCCESS
        task_repo.update(task, {
            "status": TaskStatus.SUCCESS,
            "completed_at": datetime.utcnow(),
            "result_id": result.id
        })

        logger.info(f"Successfully scraped {url}")
        return {"status": "success", "result_id": str(result.id)}

    except Exception as e:
        logger.error(f"Error scraping {url}: {e}")

        # Update task status to FAILED
        task = task_repo.get_by_id(UUID(task_id))
        if task:
            task_repo.update(task, {
                "status": TaskStatus.FAILED,
                "completed_at": datetime.utcnow(),
                "error_message": str(e)
            })

        # Retry if not max retries reached
        raise self.retry(exc=e, countdown=60)

    finally:
        db.close()
```

**éªŒè¯**:

- æˆåŠŸå¯¼å…¥: `from app.celery_app.tasks.scrape_task import scrape_website_task`

**å®Œæˆæ ‡å¿—**: Celery æŠ“å–ä»»åŠ¡å·²åˆ›å»º

---

### Task 26: åˆ›å»º Celery Worker å…¥å£ (app/celery_app/worker.py)

**ç›®æ ‡**: åˆ›å»º Celery worker å¯åŠ¨æ–‡ä»¶

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `app/celery_app/worker.py`

**ä»£ç ç»“æ„**:

```python
"""
Celery worker entry point.

To start the worker, run:
    celery -A app.celery_app.worker worker --loglevel=info
"""

from app.celery_app.celery_config import celery_app

# Import all tasks to register them
from app.celery_app.tasks import scrape_task

__all__ = ["celery_app"]
```

**éªŒè¯**:

- å¯ä»¥å¯åŠ¨ workerï¼ˆå¦‚æœ Redis è¿è¡Œä¸­ï¼‰: `celery -A app.celery_app.worker worker --loglevel=info`

**å®Œæˆæ ‡å¿—**: Celery worker å…¥å£å·²åˆ›å»º

---

### Task 27: é›†æˆ Celery åˆ° API - æ›´æ–°åˆ›å»ºä»»åŠ¡ç«¯ç‚¹

**ç›®æ ‡**: åœ¨åˆ›å»ºä»»åŠ¡æ—¶è§¦å‘ Celery ä»»åŠ¡

**å…·ä½“æ­¥éª¤**:

1. ç¼–è¾‘ `app/api/v1/endpoints/tasks.py`
2. åœ¨ `create_task` å‡½æ•°ä¸­æ·»åŠ  Celery ä»»åŠ¡è§¦å‘

**æ›´æ–°ä»£ç ** (æ›¿æ¢ create_task å‡½æ•°):

```python
from app.celery_app.tasks.scrape_task import scrape_website_task

@router.post("/tasks", response_model=TaskResponse, status_code=status.HTTP_201_CREATED, tags=["Tasks"])
async def create_task(
    task_in: TaskCreate,
    task_repo: TaskRepository = Depends(get_task_repository)
):
    """
    Create a new scraping task and trigger async processing.

    - **url**: The URL to scrape

    Returns the created task with status PENDING.
    The scraping will be performed asynchronously by Celery worker.
    """
    try:
        # Create task data
        task_data = {
            "url": str(task_in.url),
            "status": "PENDING"
        }

        # Create task in database
        task = task_repo.create(task_data)

        # Trigger Celery task asynchronously
        scrape_website_task.delay(str(task.id), str(task.url))

        return task
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to create task: {str(e)}"
        )
```

**éªŒè¯**:

- é‡å¯ FastAPI åº”ç”¨
- ç¡®ä¿ Celery worker è¿è¡Œä¸­
- åˆ›å»ºä¸€ä¸ªä»»åŠ¡ï¼Œè§‚å¯Ÿ worker æ—¥å¿—

**å®Œæˆæ ‡å¿—**: API æˆåŠŸè§¦å‘ Celery ä»»åŠ¡

---

## é˜¶æ®µ 7: Redis ç¼“å­˜å±‚ (Tasks 28-30)

### Task 28: åˆ›å»º Redis å®¢æˆ·ç«¯ (app/core/redis_client.py)

**ç›®æ ‡**: åˆ›å»º Redis è¿æ¥å’Œå®¢æˆ·ç«¯

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `app/core/redis_client.py`

**ä»£ç ç»“æ„**:

```python
import redis
from app.config import settings
from typing import Optional
import json

class RedisClient:
    """Redis client wrapper"""

    def __init__(self):
        self.client = redis.Redis(
            host=settings.REDIS_HOST,
            port=settings.REDIS_PORT,
            decode_responses=True
        )

    def get(self, key: str) -> Optional[str]:
        """Get value by key"""
        return self.client.get(key)

    def set(self, key: str, value: str, ttl: Optional[int] = None):
        """Set value with optional TTL"""
        if ttl:
            self.client.setex(key, ttl, value)
        else:
            self.client.set(key, value)

    def delete(self, key: str):
        """Delete key"""
        self.client.delete(key)

    def exists(self, key: str) -> bool:
        """Check if key exists"""
        return bool(self.client.exists(key))

    def ping(self) -> bool:
        """Test connection"""
        try:
            return self.client.ping()
        except:
            return False

# Global Redis client instance
redis_client = RedisClient()

def get_redis() -> RedisClient:
    """Dependency function to get Redis client"""
    return redis_client
```

**éªŒè¯**:

- æˆåŠŸå¯¼å…¥: `from app.core.redis_client import redis_client`
- æµ‹è¯•è¿æ¥ï¼ˆå¦‚æœ Redis è¿è¡Œä¸­ï¼‰: `redis_client.ping()`

**å®Œæˆæ ‡å¿—**: Redis å®¢æˆ·ç«¯å·²åˆ›å»º

---

### Task 29: åˆ›å»ºç¼“å­˜æœåŠ¡ (app/services/cache_service.py)

**ç›®æ ‡**: å®ç°ç¼“å­˜é€»è¾‘å°è£…

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `app/services/cache_service.py`

**ä»£ç ç»“æ„**:

```python
import hashlib
import json
from typing import Optional, Dict, Any
from app.core.redis_client import RedisClient
from app.config import settings
import logging

logger = logging.getLogger(__name__)

class CacheService:
    """Service for caching scraped results"""

    def __init__(self, redis_client: RedisClient):
        self.redis = redis_client
        self.ttl = settings.CACHE_TTL

    def get_cache_key(self, url: str) -> str:
        """Generate cache key from URL"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        return f"cache:url:{url_hash}"

    def get_cached_result(self, url: str) -> Optional[Dict[Any, Any]]:
        """Get cached result for URL"""
        try:
            cache_key = self.get_cache_key(url)
            cached_data = self.redis.get(cache_key)

            if cached_data:
                logger.info(f"Cache HIT for {url}")
                return json.loads(cached_data)

            logger.info(f"Cache MISS for {url}")
            return None

        except Exception as e:
            logger.error(f"Cache get error: {e}")
            return None

    def set_cached_result(self, url: str, result: Dict[Any, Any]):
        """Cache result for URL"""
        try:
            cache_key = self.get_cache_key(url)
            cached_data = json.dumps(result)
            self.redis.set(cache_key, cached_data, ttl=self.ttl)
            logger.info(f"Cached result for {url} (TTL: {self.ttl}s)")

        except Exception as e:
            logger.error(f"Cache set error: {e}")

    def invalidate_cache(self, url: str):
        """Invalidate cache for URL"""
        try:
            cache_key = self.get_cache_key(url)
            self.redis.delete(cache_key)
            logger.info(f"Invalidated cache for {url}")

        except Exception as e:
            logger.error(f"Cache invalidation error: {e}")
```

**éªŒè¯**:

- æˆåŠŸå¯¼å…¥: `from app.services.cache_service import CacheService`

**å®Œæˆæ ‡å¿—**: ç¼“å­˜æœåŠ¡å·²åˆ›å»º

---

### Task 30: é›†æˆç¼“å­˜åˆ° API å’Œ Celery ä»»åŠ¡

**ç›®æ ‡**: åœ¨åˆ›å»ºä»»åŠ¡å‰æ£€æŸ¥ç¼“å­˜ï¼ŒæŠ“å–åå†™å…¥ç¼“å­˜

**å…·ä½“æ­¥éª¤**:

**æ­¥éª¤ 1**: æ›´æ–° `app/api/v1/dependencies.py`ï¼Œæ·»åŠ ç¼“å­˜æœåŠ¡ä¾èµ–

```python
from app.services.cache_service import CacheService
from app.core.redis_client import get_redis, RedisClient

def get_cache_service(redis: RedisClient = Depends(get_redis)) -> CacheService:
    """Dependency to get CacheService instance"""
    return CacheService(redis)
```

**æ­¥éª¤ 2**: æ›´æ–° `app/api/v1/endpoints/tasks.py`ï¼Œåœ¨åˆ›å»ºä»»åŠ¡å‰æ£€æŸ¥ç¼“å­˜

```python
from app.services.cache_service import CacheService
from app.api.v1.dependencies import get_cache_service

# Update create_task function
@router.post("/tasks", response_model=TaskResponse, status_code=status.HTTP_201_CREATED, tags=["Tasks"])
async def create_task(
    task_in: TaskCreate,
    task_repo: TaskRepository = Depends(get_task_repository),
    cache_service: CacheService = Depends(get_cache_service)
):
    """
    Create a new scraping task and trigger async processing.
    Checks cache first to avoid duplicate scraping.
    """
    try:
        url = str(task_in.url)

        # Check cache first
        cached_result = cache_service.get_cached_result(url)
        if cached_result:
            # Return cached result as a completed task
            # (Simplified: in production, you might return the cached result differently)
            pass

        # Create task data
        task_data = {
            "url": url,
            "status": "PENDING"
        }

        # Create task in database
        task = task_repo.create(task_data)

        # Trigger Celery task asynchronously
        scrape_website_task.delay(str(task.id), url)

        return task
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to create task: {str(e)}"
        )
```

**æ­¥éª¤ 3**: æ›´æ–° `app/celery_app/tasks/scrape_task.py`ï¼ŒæŠ“å–åå†™å…¥ç¼“å­˜

```python
# Add at the top
from app.core.redis_client import redis_client
from app.services.cache_service import CacheService

# Update scrape_website_task function (add after saving result):
        # ... (existing code to save result)

        # Cache the result
        cache_service = CacheService(redis_client)
        cache_data = {
            "title": scrape_result.get("title"),
            "description": scrape_result.get("description"),
            "links": scrape_result.get("links", []),
            "result_id": str(result.id)
        }
        cache_service.set_cached_result(url, cache_data)

        # ... (rest of the code)
```

**éªŒè¯**:

- åˆ›å»ºä»»åŠ¡ä¸¤æ¬¡ï¼Œç¬¬äºŒæ¬¡åº”è¯¥æ›´å¿«ï¼ˆå¦‚æœä½¿ç”¨ç¼“å­˜ï¼‰
- æ£€æŸ¥ Redis ä¸­çš„ç¼“å­˜é”®

**å®Œæˆæ ‡å¿—**: ç¼“å­˜å·²é›†æˆåˆ° API å’Œ Celery ä»»åŠ¡

---

## é˜¶æ®µ 8: Docker åŒ– (Tasks 31-34)

### Task 31: åˆ›å»º Web åº”ç”¨ Dockerfile (docker/Dockerfile.web)

**ç›®æ ‡**: ä¸º FastAPI åº”ç”¨åˆ›å»º Docker é•œåƒ

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `docker/` ç›®å½•
2. åˆ›å»º `docker/Dockerfile.web`

**ä»£ç å†…å®¹**:

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Expose port
EXPOSE 8000

# Run application
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**éªŒè¯**:

- æ–‡ä»¶å­˜åœ¨äº `docker/Dockerfile.web`

**å®Œæˆæ ‡å¿—**: Web Dockerfile å·²åˆ›å»º

---

### Task 32: åˆ›å»º Worker Dockerfile (docker/Dockerfile.worker)

**ç›®æ ‡**: ä¸º Celery Worker åˆ›å»º Docker é•œåƒ

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `docker/Dockerfile.worker`

**ä»£ç å†…å®¹**:

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Run Celery worker
CMD ["celery", "-A", "app.celery_app.worker", "worker", "--loglevel=info"]
```

**éªŒè¯**:

- æ–‡ä»¶å­˜åœ¨äº `docker/Dockerfile.worker`

**å®Œæˆæ ‡å¿—**: Worker Dockerfile å·²åˆ›å»º

---

### Task 33: åˆ›å»º .dockerignore æ–‡ä»¶

**ç›®æ ‡**: ä¼˜åŒ– Docker æ„å»ºï¼Œå¿½ç•¥ä¸å¿…è¦çš„æ–‡ä»¶

**å…·ä½“æ­¥éª¤**:

1. åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.dockerignore`

**ä»£ç å†…å®¹**:

```
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
ENV/

# Environment
.env
.env.local

# IDE
.vscode/
.idea/
*.swp
*.swo

# Git
.git/
.gitignore

# Documentation
*.md
docs/

# Testing
.pytest_cache/
.coverage
htmlcov/
tests/

# Logs
*.log
logs/

# Database
*.db
*.sqlite
```

**éªŒè¯**:

- `.dockerignore` æ–‡ä»¶å­˜åœ¨

**å®Œæˆæ ‡å¿—**: .dockerignore å·²åˆ›å»º

---

### Task 34: åˆ›å»º docker-compose.yml

**ç›®æ ‡**: å®šä¹‰å®Œæ•´çš„å¤šæœåŠ¡ Docker Compose é…ç½®

**å…·ä½“æ­¥éª¤**:

1. åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `docker-compose.yml`

**ä»£ç å†…å®¹**:

```yaml
version: "3.8"

services:
  db:
    image: postgres:15-alpine
    container_name: webprobe_db
    environment:
      POSTGRES_USER: webprobe
      POSTGRES_PASSWORD: secret
      POSTGRES_DB: webprobe_db
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U webprobe"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - webprobe_network

  redis:
    image: redis:7-alpine
    container_name: webprobe_redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    networks:
      - webprobe_network

  web:
    build:
      context: .
      dockerfile: docker/Dockerfile.web
    container_name: webprobe_web
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    volumes:
      - .:/app
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://webprobe:secret@db:5432/webprobe_db
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - DEBUG=true
      - LOG_LEVEL=INFO
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - webprobe_network

  worker:
    build:
      context: .
      dockerfile: docker/Dockerfile.worker
    container_name: webprobe_worker
    command: celery -A app.celery_app.worker worker --loglevel=info
    volumes:
      - .:/app
    environment:
      - DATABASE_URL=postgresql://webprobe:secret@db:5432/webprobe_db
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
    depends_on:
      - db
      - redis
    networks:
      - webprobe_network

  flower:
    build:
      context: .
      dockerfile: docker/Dockerfile.worker
    container_name: webprobe_flower
    command: celery -A app.celery_app.worker flower --port=5555
    ports:
      - "5555:5555"
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
    depends_on:
      - redis
      - worker
    networks:
      - webprobe_network

volumes:
  postgres_data:

networks:
  webprobe_network:
    driver: bridge
```

**éªŒè¯**:

- è¿è¡Œ `docker-compose config` éªŒè¯è¯­æ³•
- è¿è¡Œ `docker-compose up -d` å¯åŠ¨æ‰€æœ‰æœåŠ¡
- è®¿é—® `http://localhost:8000/docs` éªŒè¯ API
- è®¿é—® `http://localhost:5555` éªŒè¯ Flower é¢æ¿

**å®Œæˆæ ‡å¿—**: docker-compose.yml å·²åˆ›å»ºï¼Œæ‰€æœ‰æœåŠ¡å¯ä»¥å¯åŠ¨

---

## é˜¶æ®µ 9: æµ‹è¯•ä¸æ–‡æ¡£ (Tasks 35-40)

### Task 35: åˆ›å»º pytest é…ç½®æ–‡ä»¶

**ç›®æ ‡**: é…ç½® pytest æµ‹è¯•æ¡†æ¶

**å…·ä½“æ­¥éª¤**:

1. åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `pytest.ini`

**ä»£ç å†…å®¹**:

```ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts =
    -v
    --strict-markers
    --tb=short
    --disable-warnings
markers =
    unit: Unit tests
    integration: Integration tests
    e2e: End-to-end tests
```

2. æ›´æ–° `requirements.txt` æ·»åŠ æµ‹è¯•ä¾èµ–:

```
pytest==7.4.3
pytest-asyncio==0.21.1
httpx==0.25.2
```

**éªŒè¯**:

- æ–‡ä»¶å­˜åœ¨
- å¯ä»¥è¿è¡Œ `pytest --collect-only`

**å®Œæˆæ ‡å¿—**: pytest é…ç½®å·²åˆ›å»º

---

### Task 36: åˆ›å»º pytest fixtures (tests/conftest.py)

**ç›®æ ‡**: åˆ›å»ºæµ‹è¯•æ‰€éœ€çš„é€šç”¨ fixtures

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `tests/conftest.py`

**ä»£ç ç»“æ„**:

```python
import pytest
from fastapi.testclient import TestClient
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from app.main import app
from app.core.database import get_db
from app.models.base import Base

# Test database URL (use SQLite for simplicity)
SQLALCHEMY_TEST_DATABASE_URL = "sqlite:///./test.db"

@pytest.fixture(scope="session")
def test_engine():
    """Create test database engine"""
    engine = create_engine(
        SQLALCHEMY_TEST_DATABASE_URL,
        connect_args={"check_same_thread": False}
    )
    Base.metadata.create_all(bind=engine)
    yield engine
    Base.metadata.drop_all(bind=engine)

@pytest.fixture(scope="function")
def test_db(test_engine):
    """Create test database session"""
    TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=test_engine)
    db = TestingSessionLocal()
    try:
        yield db
    finally:
        db.rollback()
        db.close()

@pytest.fixture(scope="function")
def client(test_db):
    """Create test client with dependency override"""
    def override_get_db():
        try:
            yield test_db
        finally:
            pass

    app.dependency_overrides[get_db] = override_get_db

    with TestClient(app) as test_client:
        yield test_client

    app.dependency_overrides.clear()
```

**éªŒè¯**:

- æ–‡ä»¶å¯ä»¥å¯¼å…¥
- è¿è¡Œ `pytest --collect-only` åº”è¯¥æˆåŠŸ

**å®Œæˆæ ‡å¿—**: Test fixtures å·²åˆ›å»º

---

### Task 37: åˆ›å»º API å•å…ƒæµ‹è¯• (tests/unit/test_api.py)

**ç›®æ ‡**: ä¸º API ç«¯ç‚¹ç¼–å†™å•å…ƒæµ‹è¯•

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `tests/unit/test_api.py`

**ä»£ç ç»“æ„**:

```python
import pytest
from fastapi import status

@pytest.mark.unit
def test_root_endpoint(client):
    """Test root endpoint"""
    response = client.get("/")
    assert response.status_code == status.HTTP_200_OK
    assert "message" in response.json()

@pytest.mark.unit
def test_health_check(client):
    """Test health check endpoint"""
    response = client.get("/api/v1/health")
    assert response.status_code == status.HTTP_200_OK
    data = response.json()
    assert data["status"] == "healthy"
    assert "database" in data

@pytest.mark.unit
def test_create_task(client):
    """Test task creation endpoint"""
    task_data = {"url": "https://example.com"}
    response = client.post("/api/v1/tasks", json=task_data)
    assert response.status_code == status.HTTP_201_CREATED
    data = response.json()
    assert "id" in data
    assert data["url"] == task_data["url"]
    assert data["status"] == "PENDING"

@pytest.mark.unit
def test_get_task(client):
    """Test get task endpoint"""
    # First create a task
    task_data = {"url": "https://example.com"}
    create_response = client.post("/api/v1/tasks", json=task_data)
    task_id = create_response.json()["id"]

    # Then retrieve it
    response = client.get(f"/api/v1/tasks/{task_id}")
    assert response.status_code == status.HTTP_200_OK
    data = response.json()
    assert data["id"] == task_id
    assert data["url"] == task_data["url"]

@pytest.mark.unit
def test_get_task_not_found(client):
    """Test get non-existent task"""
    fake_id = "00000000-0000-0000-0000-000000000000"
    response = client.get(f"/api/v1/tasks/{fake_id}")
    assert response.status_code == status.HTTP_404_NOT_FOUND

@pytest.mark.unit
def test_list_tasks(client):
    """Test list tasks endpoint"""
    # Create a few tasks
    for i in range(3):
        client.post("/api/v1/tasks", json={"url": f"https://example{i}.com"})

    # List tasks
    response = client.get("/api/v1/tasks")
    assert response.status_code == status.HTTP_200_OK
    data = response.json()
    assert "tasks" in data
    assert len(data["tasks"]) >= 3
    assert "total" in data
```

**éªŒè¯**:

- è¿è¡Œæµ‹è¯•: `pytest tests/unit/test_api.py -v`
- æ‰€æœ‰æµ‹è¯•åº”è¯¥é€šè¿‡

**å®Œæˆæ ‡å¿—**: API å•å…ƒæµ‹è¯•å·²åˆ›å»ºå¹¶é€šè¿‡

---

### Task 38: åˆ›å»º Repository å•å…ƒæµ‹è¯• (tests/unit/test_repositories.py)

**ç›®æ ‡**: ä¸º Repository å±‚ç¼–å†™å•å…ƒæµ‹è¯•

**å…·ä½“æ­¥éª¤**:

1. åˆ›å»º `tests/unit/test_repositories.py`

**ä»£ç ç»“æ„**:

```python
import pytest
from app.repositories.task_repository import TaskRepository
from app.models.task import Task, TaskStatus
from uuid import uuid4

@pytest.mark.unit
def test_create_task(test_db):
    """Test creating a task"""
    repo = TaskRepository(test_db)
    task_data = {
        "url": "https://example.com",
        "status": TaskStatus.PENDING
    }
    task = repo.create(task_data)

    assert task.id is not None
    assert task.url == task_data["url"]
    assert task.status == TaskStatus.PENDING

@pytest.mark.unit
def test_get_task_by_id(test_db):
    """Test retrieving task by ID"""
    repo = TaskRepository(test_db)

    # Create task
    task_data = {"url": "https://example.com", "status": TaskStatus.PENDING}
    created_task = repo.create(task_data)

    # Retrieve task
    retrieved_task = repo.get_by_id(created_task.id)
    assert retrieved_task is not None
    assert retrieved_task.id == created_task.id
    assert retrieved_task.url == created_task.url

@pytest.mark.unit
def test_get_task_by_url(test_db):
    """Test retrieving task by URL"""
    repo = TaskRepository(test_db)

    url = "https://unique-test-url.com"
    task_data = {"url": url, "status": TaskStatus.PENDING}
    repo.create(task_data)

    task = repo.get_by_url(url)
    assert task is not None
    assert task.url == url

@pytest.mark.unit
def test_update_task_status(test_db):
    """Test updating task status"""
    repo = TaskRepository(test_db)

    # Create task
    task_data = {"url": "https://example.com", "status": TaskStatus.PENDING}
    task = repo.create(task_data)

    # Update status
    updated_task = repo.update_status(task.id, TaskStatus.PROCESSING)
    assert updated_task.status == TaskStatus.PROCESSING

@pytest.mark.unit
def test_get_tasks_by_status(test_db):
    """Test filtering tasks by status"""
    repo = TaskRepository(test_db)

    # Create tasks with different statuses
    repo.create({"url": "https://example1.com", "status": TaskStatus.PENDING})
    repo.create({"url": "https://example2.com", "status": TaskStatus.PENDING})
    repo.create({"url": "https://example3.com", "status": TaskStatus.SUCCESS})

    # Get pending tasks
    pending_tasks = repo.get_by_status(TaskStatus.PENDING)
    assert len(pending_tasks) >= 2
    assert all(t.status == TaskStatus.PENDING for t in pending_tasks)
```

**éªŒè¯**:

- è¿è¡Œæµ‹è¯•: `pytest tests/unit/test_repositories.py -v`
- æ‰€æœ‰æµ‹è¯•åº”è¯¥é€šè¿‡

**å®Œæˆæ ‡å¿—**: Repository å•å…ƒæµ‹è¯•å·²åˆ›å»ºå¹¶é€šè¿‡

---

### Task 39: åˆ›å»º README.md

**ç›®æ ‡**: åˆ›å»ºé¡¹ç›®è¯´æ˜æ–‡æ¡£

**å…·ä½“æ­¥éª¤**:

1. åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»ºæˆ–æ›´æ–° `README.md`

**ä»£ç å†…å®¹**:

````markdown
# WebProbe - å¼‚æ­¥ Web å†…å®¹æŠ“å–ä¸åˆ†æå¹³å°

WebProbe æ˜¯ä¸€ä¸ªåŸºäº FastAPI å’Œ Celery çš„å¼‚æ­¥ç½‘é¡µæŠ“å–å¹³å°ï¼Œæ”¯æŒå¤§è§„æ¨¡å¹¶å‘æŠ“å–ã€æ™ºèƒ½ç¼“å­˜å’Œä»»åŠ¡ç®¡ç†ã€‚

## æŠ€æœ¯æ ˆ

- **Web æ¡†æ¶**: FastAPI
- **å¼‚æ­¥ä»»åŠ¡**: Celery
- **æ¶ˆæ¯é˜Ÿåˆ—**: Redis
- **æ•°æ®åº“**: PostgreSQL
- **å®¹å™¨åŒ–**: Docker & Docker Compose
- **æµ‹è¯•**: Pytest

## åŠŸèƒ½ç‰¹æ€§

- âœ… RESTful API æ¥å£
- âœ… å¼‚æ­¥ä»»åŠ¡å¤„ç†
- âœ… æ™ºèƒ½ç¼“å­˜æœºåˆ¶
- âœ… ä»»åŠ¡çŠ¶æ€è¿½è¸ª
- âœ… Docker ä¸€é”®éƒ¨ç½²
- âœ… å®Œæ•´çš„å•å…ƒæµ‹è¯•

## å¿«é€Ÿå¼€å§‹

### æ–¹å¼ 1: ä½¿ç”¨ Docker Compose (æ¨è)

1. å…‹éš†é¡¹ç›®

```bash
git clone <repository-url>
cd WebProbe
```
````

2. å¯åŠ¨æ‰€æœ‰æœåŠ¡

```bash
docker-compose up -d
```

3. åˆå§‹åŒ–æ•°æ®åº“

```bash
docker-compose exec web python scripts/init_db.py
```

4. è®¿é—®æœåŠ¡

- API æ–‡æ¡£: http://localhost:8000/docs
- Flower ç›‘æ§: http://localhost:5555

### æ–¹å¼ 2: æœ¬åœ°å¼€å‘

1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

2. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

3. é…ç½®ç¯å¢ƒå˜é‡

```bash
cp .env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶é…ç½®æ•°æ®åº“å’Œ Redis è¿æ¥
```

4. å¯åŠ¨ PostgreSQL å’Œ Redis

```bash
# ä½¿ç”¨ Docker
docker run -d -p 5432:5432 -e POSTGRES_PASSWORD=secret postgres:15-alpine
docker run -d -p 6379:6379 redis:7-alpine
```

5. åˆå§‹åŒ–æ•°æ®åº“

```bash
python scripts/init_db.py
```

6. å¯åŠ¨æœåŠ¡

```bash
# Terminal 1: FastAPI
uvicorn app.main:app --reload

# Terminal 2: Celery Worker
celery -A app.celery_app.worker worker --loglevel=info

# Terminal 3 (å¯é€‰): Flower ç›‘æ§
celery -A app.celery_app.worker flower
```

## API ä½¿ç”¨ç¤ºä¾‹

### åˆ›å»ºæŠ“å–ä»»åŠ¡

```bash
curl -X POST "http://localhost:8000/api/v1/tasks" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com"}'
```

### æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€

```bash
curl "http://localhost:8000/api/v1/tasks/{task_id}"
```

### åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡

```bash
curl "http://localhost:8000/api/v1/tasks?skip=0&limit=10"
```

## æµ‹è¯•

è¿è¡Œæ‰€æœ‰æµ‹è¯•:

```bash
pytest
```

è¿è¡Œç‰¹å®šç±»å‹çš„æµ‹è¯•:

```bash
pytest -m unit          # å•å…ƒæµ‹è¯•
pytest -m integration   # é›†æˆæµ‹è¯•
```

## é¡¹ç›®ç»“æ„

è¯¦è§ [architecture.md](architecture.md) äº†è§£å®Œæ•´çš„æ¶æ„è®¾è®¡ã€‚

## å¼€å‘è®¡åˆ’

è¯¦è§ [tasks.md](tasks.md) äº†è§£ MVP å¼€å‘ä»»åŠ¡åˆ—è¡¨ã€‚

## License

MIT

````

**éªŒè¯**:
- README.md å­˜åœ¨ä¸”å†…å®¹å®Œæ•´

**å®Œæˆæ ‡å¿—**: README.md å·²åˆ›å»º

---

### Task 40: åˆ›å»ºå¯åŠ¨è„šæœ¬ (scripts/start.sh)
**ç›®æ ‡**: åˆ›å»ºä¾¿æ·çš„å¯åŠ¨è„šæœ¬

**å…·ä½“æ­¥éª¤**:
1. åˆ›å»º `scripts/start.sh`

**ä»£ç å†…å®¹**:
```bash
#!/bin/bash

# WebProbe startup script

echo "ğŸš€ Starting WebProbe..."

# Check if Docker is installed
if ! command -v docker &> /dev/null; then
    echo "âŒ Docker is not installed. Please install Docker first."
    exit 1
fi

# Check if Docker Compose is installed
if ! command -v docker-compose &> /dev/null; then
    echo "âŒ Docker Compose is not installed. Please install Docker Compose first."
    exit 1
fi

# Start services
echo "ğŸ“¦ Starting Docker services..."
docker-compose up -d

# Wait for services to be ready
echo "â³ Waiting for services to be ready..."
sleep 10

# Initialize database
echo "ğŸ—„ï¸  Initializing database..."
docker-compose exec -T web python scripts/init_db.py

echo "âœ… WebProbe is running!"
echo ""
echo "ğŸ“ Access points:"
echo "   - API Documentation: http://localhost:8000/docs"
echo "   - API Root: http://localhost:8000"
echo "   - Flower Dashboard: http://localhost:5555"
echo ""
echo "ğŸ“Š Check service status:"
echo "   docker-compose ps"
echo ""
echo "ğŸ›‘ Stop services:"
echo "   docker-compose down"
````

2. åˆ›å»º `scripts/stop.sh`

```bash
#!/bin/bash

echo "ğŸ›‘ Stopping WebProbe..."
docker-compose down
echo "âœ… All services stopped."
```

3. ä½¿è„šæœ¬å¯æ‰§è¡Œ

```bash
chmod +x scripts/start.sh
chmod +x scripts/stop.sh
```

**éªŒè¯**:

- è„šæœ¬æ–‡ä»¶å­˜åœ¨ä¸”å¯æ‰§è¡Œ
- å¯ä»¥è¿è¡Œ `./scripts/start.sh`

**å®Œæˆæ ‡å¿—**: å¯åŠ¨è„šæœ¬å·²åˆ›å»º

---

## ğŸ‰ MVP å®Œæˆæ£€æŸ¥æ¸…å•

å®Œæˆæ‰€æœ‰ 40 ä¸ªä»»åŠ¡åï¼Œä½ åº”è¯¥æœ‰ä¸€ä¸ªå®Œæ•´çš„ MVPï¼ŒåŒ…æ‹¬:

### âœ… æ ¸å¿ƒåŠŸèƒ½

- [x] RESTful API æ¥å£ (åˆ›å»ºä»»åŠ¡ã€æŸ¥è¯¢ä»»åŠ¡ã€åˆ—å‡ºä»»åŠ¡)
- [x] å¼‚æ­¥ä»»åŠ¡å¤„ç† (Celery + Redis)
- [x] ç½‘é¡µæŠ“å–åŠŸèƒ½ (requests + BeautifulSoup)
- [x] æ•°æ®åº“å­˜å‚¨ (PostgreSQL + SQLAlchemy)
- [x] Redis ç¼“å­˜æœºåˆ¶

### âœ… åŸºç¡€è®¾æ–½

- [x] Docker å®¹å™¨åŒ–
- [x] Docker Compose ç¼–æ’
- [x] ç¯å¢ƒé…ç½®ç®¡ç†
- [x] æ•°æ®åº“è¿ç§»æ”¯æŒ

### âœ… ä»£ç è´¨é‡

- [x] Repository æ¨¡å¼
- [x] Service æ¨¡å¼
- [x] ä¾èµ–æ³¨å…¥
- [x] å•å…ƒæµ‹è¯•
- [x] API æ–‡æ¡£ (è‡ªåŠ¨ç”Ÿæˆ)

### âœ… ç›‘æ§ä¸è¿ç»´

- [x] å¥åº·æ£€æŸ¥ç«¯ç‚¹
- [x] Flower ä»»åŠ¡ç›‘æ§
- [x] ç»“æ„åŒ–æ—¥å¿—
- [x] å¯åŠ¨è„šæœ¬

## éªŒè¯ MVP

è¿è¡Œä»¥ä¸‹å‘½ä»¤éªŒè¯ MVP æ˜¯å¦å®Œæ•´å·¥ä½œ:

```bash
# 1. å¯åŠ¨æ‰€æœ‰æœåŠ¡
./scripts/start.sh

# 2. è¿è¡Œæµ‹è¯•
pytest -v

# 3. æµ‹è¯• API
curl -X POST http://localhost:8000/api/v1/tasks \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com"}'

# 4. æ£€æŸ¥ Celery ä»»åŠ¡
# è®¿é—® http://localhost:5555

# 5. æŸ¥çœ‹ API æ–‡æ¡£
# è®¿é—® http://localhost:8000/docs
```

## ä¸‹ä¸€æ­¥æ‰©å±• (å¯é€‰)

å®Œæˆ MVP åï¼Œå¯ä»¥è€ƒè™‘ä»¥ä¸‹æ‰©å±•:

1. **è®¤è¯ä¸æˆæƒ**: JWT æˆ– API Key
2. **é™æµ**: Redis é™æµä¸­é—´ä»¶
3. **Webhook**: ä»»åŠ¡å®Œæˆé€šçŸ¥
4. **æ‰¹é‡ä»»åŠ¡**: æ‰¹é‡åˆ›å»ºå’Œç®¡ç†ä»»åŠ¡
5. **ç»“æœå¯¼å‡º**: CSV/JSON å¯¼å‡º
6. **å®šæ—¶ä»»åŠ¡**: Celery Beat å®šæ—¶æŠ“å–
7. **ç›‘æ§å‘Šè­¦**: é›†æˆ Prometheus + Grafana
8. **å‰ç«¯ç•Œé¢**: Vue.js æˆ– React å‰ç«¯

---

**ç¥ä½ æ„å»ºé¡ºåˆ©ï¼ğŸš€**
